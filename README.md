# insideLLMs
Python library for probing the inner workings of large language models. Systematically test LLMs' zero-shot ability at unseen logic problems, propensity for bias, and vulnerabilities to particular attacks involving recursion, reframing and tokenization. 
