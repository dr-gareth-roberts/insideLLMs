[project]
name = "insideLLMs"
version = "0.2.0"
description = "Cross-model behavioural probe harness for LLM evaluation and comparison."
readme = { file = "README.md", content-type = "text/markdown" }
requires-python = ">=3.10"
license = { file = "LICENSE" }
authors = [
    { name = "insideLLMs Contributors" }
]
keywords = [
    "llm",
    "language-models",
    "nlp",
    "machine-learning",
    "ai",
    "evaluation",
    "benchmarking",
    "testing",
    "probing",
    "harness",
    "comparison",
    "behavioural",
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Text Processing :: Linguistic",
    "Typing :: Typed",
]

dependencies = [
    "pyyaml>=6.0",
]

[project.optional-dependencies]
# Provider extras — install only the providers you need
openai = [
    "openai>=1.0.0",
]
anthropic = [
    "anthropic>=0.18.0",
]
huggingface = [
    "transformers>=4.30.0",
    "huggingface-hub>=0.16.0",
]
providers = [
    "insideLLMs[openai,anthropic,huggingface]",
]
# Feature extras
signing = [
    "tuf>=3.0.0",
    "oras>=0.2.0",
]
crypto = [
    "cryptography>=41.0.0",
]
nlp = [
    "nltk>=3.9.3",
    "spacy>=3.5.0",
    "scikit-learn>=1.2.0",
    "gensim>=4.3.0",
]
tokenization = [
    "tiktoken>=0.5.0",
]
visualization = [
    "matplotlib>=3.7.0",
    "pillow>=12.1.1",
    "pandas>=2.0.0",
    "seaborn>=0.12.0",
]
langchain = [
    "langchain",
    "langchain-core",
    "langgraph",
]
serving = [
    "fastapi>=0.100.0",
    "uvicorn>=0.22.0",
]
otel = [
    "opentelemetry-api>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
]
# Development
dev = [
    "pytest>=7.3.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.21.0",
    "pytest-benchmark>=4.0.0",
    "hypothesis>=6.80.0",
    "ruff>=0.1.0",
    "mypy>=1.5.0",
    "pre-commit>=3.3.0",
    "types-PyYAML>=6.0.12.20240311",
]
# Everything
all = [
    "insideLLMs[providers,signing,crypto,nlp,visualization,dev,serving,otel,tokenization]",
]

[project.scripts]
insidellms = "insideLLMs.cli:main"

[project.urls]
Homepage = "https://github.com/dr-gareth-roberts/insideLLMs"
Repository = "https://github.com/dr-gareth-roberts/insideLLMs"
Documentation = "https://dr-gareth-roberts.github.io/insideLLMs/"
"Changelog" = "https://github.com/dr-gareth-roberts/insideLLMs/blob/main/CHANGELOG.md"
"Bug Tracker" = "https://github.com/dr-gareth-roberts/insideLLMs/issues"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["insideLLMs"]

[tool.ruff]
line-length = 100
target-version = "py310"
exclude = [
    ".git",
    ".venv",
    ".backups",
    ".tmp",
    "__pycache__",
    "build",
    "dist",
]

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # Pyflakes
    "I",      # isort
]
ignore = [
    "E501",   # line too long (handled by formatter)
    "B008",   # function calls in default arguments
    "ARG001", # unused function argument
    "ARG002", # unused method argument
    "UP035",  # prefer collections.abc import (not enforced for now)
    "UP045",  # Optional[T] -> T | None (not enforced for now)
]

[tool.ruff.lint.isort]
known-first-party = ["insideLLMs"]

[tool.ruff.lint.per-file-ignores]
"examples/**/*.py" = ["ARG", "F401", "UP"]
"tests/**/*.py" = ["ARG", "B017", "E741", "F401", "SIM", "UP"]
"insideLLMs/caching.py" = ["E402"]
"insideLLMs/contrib/**/*.py" = ["ARG", "E501", "F401", "F811"]

[tool.mypy]
python_version = "3.10"
warn_return_any = false
warn_unused_configs = true
disallow_untyped_defs = false
disallow_incomplete_defs = false
check_untyped_defs = true
ignore_missing_imports = true
pretty = true
show_error_codes = true
show_column_numbers = true
# Progressively re-enabling error codes. Phase 1 keeps the most impactful
# disabled; phase 2+ will remove more from this list.
# Phase 2: re-enabled arg-type, assignment, call-arg (2026-03-02)
# Phase 3 target: re-enable union-attr, operator, index
disable_error_code = [
    "attr-defined",
    "call-overload",
    "dict-item",
    "exit-return",
    "has-type",
    "list-item",
    "operator",
    "index",
    "var-annotated",
    "no-any-return",
    "misc",
    "union-attr",
    "override",
    "type-var",
]

# Strict checking for core modules
[[tool.mypy.overrides]]
module = [
    "insideLLMs.runtime.*",
    "insideLLMs.cli.*",
    "insideLLMs.schemas.*",
    "insideLLMs._serialization",
    "insideLLMs.injection",
    "insideLLMs.safety",
]
disallow_untyped_defs = true
warn_return_any = true

# Transitional: modules with known arg-type/assignment issues (fix incrementally)
# These were surfaced when re-enabling arg-type, assignment, call-arg in Phase 2.
# Target: resolve and remove this override by Phase 3.
[[tool.mypy.overrides]]
module = [
    "insideLLMs.probes.instruction",
    "insideLLMs.probes.code",
    "insideLLMs.probes.bias",
    "insideLLMs.probes.attack",
    "insideLLMs.exceptions",
    "insideLLMs.optimization",
    "insideLLMs.structured",
    "insideLLMs.analysis.export",
    "insideLLMs.analysis.visualization",
    "insideLLMs.analysis.evaluation",
    "insideLLMs.analysis.comparison",
    "insideLLMs.logging_utils",
    "insideLLMs.semantic_cache",
    "insideLLMs.retry",
    "insideLLMs.async_utils",
    "insideLLMs.experiment_tracking",
    "insideLLMs.nlp.similarity",
    "insideLLMs.nlp.text_cleaning",
    "insideLLMs.nlp.keyword_extraction",
    "insideLLMs.models.cohere",
    "insideLLMs.models.__init__",
    "insideLLMs.runtime._sync_runner",
    "insideLLMs.runtime._async_runner",
    "insideLLMs.runtime._high_level",
    "insideLLMs.runtime._config_loader",
    "insideLLMs.runtime._result_utils",
    "insideLLMs.runtime._artifact_utils",
    "insideLLMs.runtime.observability",
    "insideLLMs.runtime.reproducibility",
    "insideLLMs.runtime.diffing",
    "insideLLMs.cli._report_builder",
    "insideLLMs.cli.commands.run",
    "insideLLMs.cli.commands.diff",
    "insideLLMs.cli.commands.compare",
    "insideLLMs.cli.commands.doctor",
    "insideLLMs.trace.tracing",
    "insideLLMs.trace.trace_contracts",
    "insideLLMs.trace.trace_config",
]
disable_error_code = ["arg-type", "assignment"]

# Lenient checking for contrib (disconnected modules)
[[tool.mypy.overrides]]
module = "insideLLMs.contrib.*"
ignore_errors = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "determinism: tests that verify deterministic artifact and diff-gating guarantees",
    "contract: tests for public stability contracts (CLI/schema/artifact contracts)",
    "adapter: tests focused on adapter/provider integration layers",
    "performance: tests focused on latency/throughput/performance behavior",
]

[tool.coverage.run]
source = ["insideLLMs"]
branch = true
omit = [
    "*/tests/*",
    "*/__pycache__/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if TYPE_CHECKING:",
    "if __name__ == .__main__.:",
]

[tool.bandit]
# Skip B101 (assert_used) — asserts are used throughout the library for invariants
skips = ["B101"]
targets = [
    "insideLLMs/injection.py",
    "insideLLMs/safety.py",
    "insideLLMs/contrib/distributed.py",
    "insideLLMs/contrib/agents.py",
    "insideLLMs/runtime/observability.py",
]

