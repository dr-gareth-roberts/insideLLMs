#!/usr/bin/env bash
set -euo pipefail

if [[ -z "${OLLAMA_API_KEY:-}" ]]; then
  echo "OLLAMA_API_KEY is required. Export it before running this script." >&2
  exit 1
fi

if [[ -z "${OLLAMA_CLOUD_MODELS:-}" ]]; then
  cat >&2 <<'EOF'
OLLAMA_CLOUD_MODELS is required.
Provide a comma- or space-separated list of Ollama Cloud models, for example:
  export OLLAMA_CLOUD_MODELS="llama3.1:cloud,qwen2.5:cloud,mistral:cloud"
EOF
  exit 1
fi

RUN_ROOT="${RUN_ROOT:-.tmp/runs/ollama-cloud}"
CONFIG_PATH="${CONFIG_PATH:-.tmp/harness_ollama_cloud_all_models.yaml}"
DATASET_PATH="${DATASET_PATH:-examples/probe_battery.jsonl}"
BASELINE_DIR="${BASELINE_DIR:-${RUN_ROOT}/baseline}"
CANDIDATE_DIR="${CANDIDATE_DIR:-${RUN_ROOT}/candidate}"

mkdir -p "$(dirname "${CONFIG_PATH}")" "${BASELINE_DIR}" "${CANDIDATE_DIR}"

IFS=', ' read -r -a MODEL_LIST <<< "${OLLAMA_CLOUD_MODELS}"

cat > "${CONFIG_PATH}" <<EOF
# Autogenerated by scripts/ollama_cloud_harness_diff.sh
models:
EOF

for model_name in "${MODEL_LIST[@]}"; do
  [[ -z "${model_name}" ]] && continue
  cat >> "${CONFIG_PATH}" <<EOF
  - type: ollama
    args:
      model_name: ${model_name}
      base_url: https://ollama.com
      timeout: 600
EOF
done

cat >> "${CONFIG_PATH}" <<'EOF'

probes:
  - type: logic
    args: {}
  - type: factuality
    args: {}
  - type: prompt_injection
    args: {}
  - type: instruction_following
    args: {}
  - type: code_generation
    args:
      language: python

dataset:
  format: jsonl
  path: ${DATASET_PATH}

max_examples: 3

generation:
  temperature: 0.0
  seed: 42
  max_tokens: 512

report_title: Ollama Cloud All-Model Benchmark
output_dir: .tmp/runs/ollama-cloud/all-models
EOF

insidellms harness "${CONFIG_PATH}" --run-dir "${BASELINE_DIR}"
insidellms harness "${CONFIG_PATH}" --run-dir "${CANDIDATE_DIR}"
insidellms diff "${BASELINE_DIR}" "${CANDIDATE_DIR}" --fail-on-changes
